\chapter{Projektbeskrivelse}
Følgende afsnit har til formål at skabe rammerne for rapportens retning igennem en problemanalyse, problemformulering og et metodeafsnit. Problemanalysen vil analysere det initerende problem og undersøge præmisserne for problemet. Dette vil efterfølgende blive brugt til klarlægning af problemformuleringen, der vil have hovedfokus på billedkomprimeringsmetoder og databehandling via lineær algebra.

\section{Problemanalyse}
%Digitale billeder er en del af mange menneskers hverdag og har for mange store virksomheder en afgørende betydingen for deres eksistens. Qua den rivende hurtige teknologiudvikling det seneste årti er digitalkameraer blevet udviklet til at have, blandt andet, højere opløsning. Den højere oplsøning betyder at den rå filstørrelse på mange fotografier er blevet betydeligt større. Dette er et problem for den enkelte bruger, der typisk besidder et stort fotoalbum, der fylder deres digitale hukommelse meget hurtigt. Det kan derfor være interessant at komprimere billedet for at få plads til flere billeder, men uden at miste kvaliteten af disse billeder. Komprimeringen skal altså helst foregå uden tab af synlig kvalitet.
%
%Problemet med store billeder strækker mere vidt end blot forbrugerens digitale enheder. Hjemmesider, og specielt billedetunge hjemmesider, bliver meget langsomme at hente, hvis ikke filstørrelsen på billederne bliver nedbragt, da store filer tager betydeligt længere tid at downloade end små filer. Billedkomprimering er specielt udbredt ved sider som Facebook, der dagligt håndterer et utal af billeder, der alle skal ligge på en server og skal deles med millioner af mennesker konstant. Her spiller størrelsen og kvaliteten af billederne en betydelig rolle.
%
%Der findes mange forskellige komprimeringsmetoder til billeder, hvoraf JPEG formentlig er den mest udbredte metode. Joint Photographic Experts Group er bagmændende bag det udbredte filformat .jpeg/.jpg, der står for bagmændende med samme initialer. JPEG er en lossy komprimeringmetode, hvor der gives tab på data mod at opnå en mindre filstørrelse. Metoden der bruges i JPEG er tilnærmelsesvis en Diskret Cosinus Transformation (DCT), der arbejder ud fra tesen om at mennesker har sværere ved at se høje frekvenser, hvorved disse med fordel kan fjernes i billedet. Desuden formrå transformationen at udtrykke mange signalværdier ved færre frekvensværdier. Der vil i følgende rapport blive arbejdet med DCT, hvilke fordele og ulemper den har såvel som teste den på et billede og vurdere kvaliteten af komprimeringen.
%En anden metode til komprimering er Principal Component Analysis (PCA), som arbejder med egenværdier. PCA simplificerer billedet i forhold til hvilke data, der har størst betydning og fjerner dermed data af mindre betydning. Denne ekskludering af data nedbringer diversiteten i billedet og dermed nedbringes filstørrelsen af det komprimerede billede.

Digitale billeder er en stor del af mange menneskers hverdag, både direkte og indirekte, og har en ligeledes en afgørende betydning for mange store teknologivirksomheders eksistens. Qua den rivende hurtige teknologiudvikling det seneste årti er digitalkameraer blevet udviklet til at have, blandt andet, højere opløsning. Højere opløsning betyder at billedfilerne også fylder betydeligt mere på en harddisk, hvorved disse hurtigere end tidligere opfyldes. Det betyder ydermere at download fra internettet bliver langsommere, da en større mængde data tager længere tid at downloade end en lille mængde data.

Den enkelte bruger, der typisk besidder et stort fotoalbum af den ene eller art, bliver påvirket af den stigende størrelse på billedfilerne. Dette gør de ved at den enkelte brugers harddisk hurtigere bliver opfyldt af billederne, og da de fleste ønsker at gemme så mange billeder som muligt, så kan det derfor være interessant at komprimere billederne for at få plads til flere billeder.
Facebook er et eksempel på en virksomhed, hvor digitale billeder har stor betydning for virksomhedens eksistens. Facebook bruger billeder som en stor del af deres brugeroplevelse, hvorved de besidder ubeskrivelige mængder billeder. For at disse billeder kan opbevares på mindst mulige plads, men også for at download-tiden for billederne, og dermed Facebooks side, holdes nede, bliver billederne komprimeret. \fixme{http://www.freedigitalphotos.net/blog/tutorials/avoiding-facebook-image-compression/}

Hele essensen i komprimering af billeder er at gøre filerne mindre, hvorved de fylder mindre på en harddisk, hvilket anses som en billedbehandling. Billedet som skal behandles kan anses som værende en $m \times n$ matrice, hvor $m$ og $n$ er billedets dimensioner i pixels. Indholdet i matricen er billedets pixels, og indeholder forskellige data afhængigt af billedformatet. Billedbehandlinger såsom komprimering fremkommer ved operationer på matricen, og er af den matematiske gren lineær algebra, der netop omhandler matrixoperationer.

Der findes mange forskellige komprimeringsmetoder, og disse kan grundlæggende opdeles i to forskellige typer; tabsfri og ikke-tabsfri komprimering. Tabsfri komprimering er en komprimering af dataene uden at ændre/slette noget, og ikke-tabsfri er det modsatte; en komprimering med tab af data.
JPEG-komprimering er formentlig den mest udbredte komprimeringsmetode, og kommer fra bagmændende med samme initialer; Joint Photographic Experts Group. Metoden brugt i JPEG er tilnærmelsesvist en \emph{Diskret Cosinus Transformation} (fremover kaldet DCT), som fungerer på baggrund af tesen om at menneskets syn ikke særlig godt ser høje frekvenser, hvorfor disse uden betydelige problemer kan fjernes fra billedet. DCT er en ikke-tabsfri komprimeringsmetode og ved dekodning af den komprimerede fil, vil det genskabte billede kun være en tilnærmelse af det originale, men ikke en eksakt kopi. Forskellen mellem det genskabte og originale billede er de høje frekvenser, og tabet ved komprimeringen burde derfor ikke være synlig for det blotte øje. DCT er datauafhængig og transformationene fungerer derved på alle billeder (dog med forskellig kvalitet).
En anden metode til komprimering er \emph{Principal Component Analysis} (fremover kaldet PCA), der ligesom DCT er en ikke-tabsfri komprimeringsmetode. PCA er en statistisk metode, der i store mængder data identificerer signifikante (og dermed også insignifikante) data. PCA finder kovariansen mellem dataene og tildeler de enkelte dimensioner en eigenvektor med dertilhørende eigenværdi, der bruges til vurdering af signifikante og insignifikante data. Modsat DCT fjerner man ikke de høje frekvenser med PCA, men derimod de data, der har mindst betydning for billedets repræsentation. Det betyder dog også at billede (ligesom DCT) blot skaber en tilnærmelse af det oprindelige billede ved dekodning. Rent beregningsmæssigt er PCA tungere end DCT, da PCA er dataafhængig, hvorved komprimeringen er dataspecifik, og ikke blot kan bruges på enhvert billede.

På trods af mange ligheder mellem PCA og DCT, fungerer de i praksis meget forskelligt, hvilket også betyder at de har hver deres fordele og ulemper. Det betyder også at hver metode fungerer bedre på nogle former for billeder end andre, hvorved det kan være svært at finde den bedst egnede metode til et givent billede.


\section{Problemformulering}
I nærkomne rapport vil digitale billeder blive komprimeret vha. lineær algebra og der vil blive undersøgt ud fra problemformuleringen:
\emph{Hvilken af komprimeringsmetoderne DCT og PCA komprimerer bedst billedet Lena\fixme{Kilde/reference}, og hvordan fungerer disse komprimeringsmetoder rent matematisk? Hvilken metode komprimerer bedst et andet tilfældigt billede, og er der nogen sammenhæng mellem disse resultater?\fixme{PCA til at vurdere vores resultatdata?}}

%redegjort for JPEG-komprimeringens underliggende matematik (Diskret Cosinus Transformation) og ydermere bruges denne metode på et farvebillede. Herefter redegøres der for matematikken bag Principal Component Analysis (PCA) og teorien anvendes på tidligere nævnte billede. Begge billeder vil blive komprimeret via Huffman coding, der omdanner billeddataene til en fil. Resultaterne vurderes på baggrund af parametre som hastighed, komprimeringsgrad og kvalitet, der alle har betydning for komprimmerings brugbarhed i virkeligheden og det konkluderes hvorvidt disse komprimeringsmetoder komprimerer billederne tilstrækkeligt i forhold til tabet af synlig data.


\section{Metode}
Trods at et utal af komprimeringsmetoder arbejdes der i følgende rapport blot med komprimeringsmetoderne DCT og PCA. Begge metoder fungerer ved at fjerne de (for det menneskelige øje) mindst relevante data. Komprimeringsmetodernes matematiske teorier vil blive undersøgt og efterføglende blive brugt til udvikling af et Pythonprogram til komprimering af et billedet. Billedet der vil blive komprimeret er Lena\fixme{kilde/reference}, som er $512 \times 512$ pixels stort og i farverummet RGB\footnote{Rød, Grøn, Blå}. Resultaterne af komprimeringerne vil blive sammenlignet i forhold til parametrene: komprimeringsgrad, hastighed\fixme{Ønsker vi stadig at måle på hastighed?(Seb)} og billedekvalitet. Komprimeringsgraden vil blive sammenlignet på baggrund af antal bits pr. pixel i det oprindelige billede i forhold til det oprindelige billede. Hastigheden sammenlignes vha. en timerfunktioner, der afgør hvor hurtige de to programmer er, og hvorvidt dette vil være brugbart i en virkelig situation. Billedkvaliteten vurderes på baggrund af forskellene i pixelværdierne mellem det oprindelige og komprimerede billede. Afslutningsvist vil komprimeringsprogrammerne bruges på flere forskellige billeder, for at afklare, hvilke parametre, der har betydning for hvilken af metoderne, der fungerer bedst.

%Der bliver i rapporten ikke blive arbejdet med alle billedkomprimeringsmetoder, men blot et udpluk af metoderne. Fokus vil blive lagt på DCT og PCA, der begge arbejder med at fjerne data der for det menneskelige øje ikke er af stor betydning. Disse metoder vil blive brugt på billedet af Lena \fixme{kilde}, der er $512 \times 512$ pixel stort og i farvespektret RGB. Dette billede vil blive komprimeret med begge metoder og resultaterne vil slutteligt blive sammenlignet i forhold til komprimeringsparametre; hastighed, komprimeringsgrad og billedkvalitet. Her spiller billedtypen også en stor rolle, hvorved at resultaterne også sammenholdes med resultaterne af andre billeder, for at fremhæve fordele og ulemper ved metoderne i forhold til de data de får som input.
