\chapter{Billedkomprimering med Principal Component Analysis}
Følgende afsnit vil undersøge den statistiske metode \emph{Principal Component Analysis} (PCA), der bruger elementer fra lineær algebra. Den statistiske baggrund såvel som baggrunden i lineær algebra vil blive klarlagt og brugt til udviklingen af et Pythonprogram, der kan komprimere et billede.

For at give et indblik i PCA og brugen af denne, tages først udgangspunkt i et eksempel, som senere generaliseres til brug af PCA på et vilkårligt datasæt. Eksemplet til brug i de følgende undersøgelser er, at en given forsker forsøger at forstå et fænomen ved at undersøge forskellige variable i et stort datasæt; datasættets variable er målinger fra et eksperiment, men der fremstår ikke nogen tydelige sammenhænge i dataene. Dette er enten fordi dataene indeholder støj, eller fordi dimensionen af datamængden er så stor, at det er uoverskueligt at se sammenhængene. \\
Tag et legetøjseksperiment som eksempel: en bold hænger i en fjeder fra loftet og hopper op og ned. Forsøget måles vha. tre kameraer placeret omkring bolden og fjederen, som tilsammen kan måle boldens position i et tredimensionelt rum, hvor hvert kamera måler to dimensioner. Uvidende om at bolden hopper lodret op og ned, måles alle tre dimensioner alligevel, men kameraerne er ikke nøvendigvis placeret i 90° i forhold til hinanden, hvorved det ikke er et standardkoordinatsystemet, der måles efter. Boldens position burde kunne repræsenteres vha. én variabel, men grundet det sløsede eksperimentsetup bliver boldens position repræsenteret ved mange variable frem for den enkelte variabel $z$. Spørgsmålet er altså; hvordan ændres et rodet datasæt (her menes: for mange variable og støjfyldt datasæt) til en simpel repræsentation vha. en variabel $z$ [\citet{PCA_slens}, s. 1-2]?

Formålet med PCA er at finde den "bedste" \ repræsentation (begrebet den bedste repræsentation vil blive uddybet i sektion \ref{sec:statistiske tools}) af et datasæt med støj og unødvendige dimensioner såvel som at afklare sammenhænge, der for det blotte øje, ikke er tydelige. Fordelene ved dette er at datasættet kan forsimples til de data, der reelt har betydning for forsøget, så eksempelvis to forskellige variable reduceres til én variabel i stedet.

Datasættet kan repræsenteres ved en $m \times n$ matrix, $X$, bestående af $n$ prøver (her: tidspunkterne kameraerne tager et billede) med $m$ målinger (her: $x(t), y(t)$ koordinaterne for de enkelte kameraer). Dermed kan \emph{søjlerne} i $X$ betegnes som vektorer, $$ \vec{z_t}=\begin{bmatrix} x_{A} & y_{A} & x_{B} & y_{B} & x_{C} & y_{C} \end{bmatrix}^T $$ hvor $t$ er tidspunktet, og indgangene er koordinaterne målt af hhv. kamera A, B og C.

\subsection*{Skift af basis}
Lad $V$ være et underrum af $\mathbb{R}^n$, så er en basis til $V$ en lineært uafhængigt udspændende mængde bestående af vektorer i $V$, hvorved alle vektorer i $V$ kan repræsenteres unikt som en lineær kombination af vektorerne i basen. \\
Standardbasen består af standardvektorerne, og det er denne basis, der normalt bruges til repræsentation af data. Der findes mange forskellige baser til et underrum, men det kan være fordelagtigt at ændre basen for et datasæt, så datasættets egenskaber fremstår tydeligere eller simplere [\citet{linalg}, s. 241-242].

Det ønskes at finde en anden basis, som er en lineær kombination af den oprindelige basis, der udtrykker data på "bedste mulige måde". Det ønskes altså at lave en lineær transformation på $m \times n$ matricen $X$ til $m \times n$ matricen $Y$ for en $m \times m$ matrix $P$, hvor $Y$ er en repræsentationen af $X$ i den nye basis $P$. Dette udtrykkes som
\begin{equation}
Y=PX
\label{eq:linear transformation}
\end{equation}
Undersøges dimensionerne af ligning \vref{eq:linear transformation} fremkommer det at
\begin{align}
(m \times n) = (m \times m)(m \times n)
\end{align}
Det gælder ydermere at ${\vec{p_{i}}}^T$ er \emph{søjlerne} i $P$, $\vec{x_{j}}$ er \emph{søjlerne} i $X$ og $\vec{y_{j}}$ er \emph{søjlerne} i $Y$. Ligning \ref{eq:linear transformation} er et skift af basis, og det gælder at $P$ er en matrix, der lineært transformerer $X$ til $Y$. Det gælder ydermere, at \emph{rækkerne} i $P = \begin{bmatrix} \vec{p_{1}} & \vec{p_{2}} & \hdots & \vec{p_{m}} \end{bmatrix}^T$ er basisvektorerne i den nye basis, der udtrykker \emph{søjlerne} i $X$.
%\emph{Søjle}vektorerne i $P$ er egenvektorerne til $Y$, og dermed
\emph{Søjlerne} $\vec{y_{j}}$ kan opskrives som
$$ Y = \begin{bmatrix}
\vec{p_1} \cdot \vec{x_1}	& \cdots		&	\vec{p_1} \cdot \vec{x_n}	\\ 
\vdots			& \ddots		&	\vdots			\\ 
\vec{p_m} \cdot \vec{x_1}	& \cdots		&	\vec{p_m} \cdot \vec{x_n}
\end{bmatrix} , \phantom{mmmm} \vec{y_{j}}=\begin{bmatrix} \vec{p_1} \cdot \vec{x_j} \\ \vdots \\ \vec{p_m} \cdot \vec{x_j} \end{bmatrix}$$

Endvidere er her tale om en ortogonal projektion af $X$ over i $P$. 
Ved projekteringen udvælges en delmængde af $P$ som nye basisvektorer som repræsenterer søjlerne af $X$ [\citet{PCA_people}, s. 6]. Den j'te koefficient af $\vec{y_j}$ er en projektion på den j'te række af $P$. Projektionen definerer reelt set et nyt ortogonalt koordinatsystem, som beskriver variansen, i dette tilfælde, et billede optimalt [\citet{PCA_slens}, s. 3].


\section{Statistiske værktøjer} \label{sec:statistiske tools}
\emph{Række}vektorerne ${\vec{p_{1}}},{\vec{p_{2}}},\ldots,{\vec{p_{m}}}$ er principale komponenter til matricen $X$, hvorved de er ortogonalt lineært uafhængige. Principale komponenter er akser, hvorved det oprindelige datasæt har nye koordinater, der sikrer at enkelte akser har stor variation [\citet{PCA_slens}, s. 5]. Som tidligere nævnt ønskes det at finde at den "bedste" \ repræsentation af $X$ såvel som at vælge et passende $P$. Svarene på disse kommer af at bestemme, hvilke egenskaber $Y$ skal have. Når egenskaberne til $Y$ er afklarede så kan det afgøres, hvilke egenskaber $P$ skal have for at danne den ønskede $Y$. Dette lægges dog lidt til side for nu, og der ses først på de overflødige data, der kan være i datasættet $X$ (dette kan være hhv. støj og redundans), og efterfølgende hvordan det ønskes at repræsentere datasættet.

\subsection{Støj} \label{sec:stoj}
Støj i et datasæt kan fremkomme af mange forskellige årsager, men vigtigst er betydningen af støjen, uanset art. Et datasæt med støj vil, efter bearbejdning, give fejlbehæftede resultater, da der vil være fejlagtige målinger, der påvirker resultatet. Det ønskes altså at minimere eller helt fjerne støjen i datasættet. En udbredt metode til at udregne støjen i et datasæt er SNR (signal-to-noise-ratio), som er et forhold mellem varianserne for hhv. signalet og støjen\footnote{Varians introduceres i afsnit \ref{sec:varians}, men kan her anses som spredningen i dataene.}. SNR er defineret som værende $SNR = \frac{\sigma^2_{\text{signal}}}{\sigma^{2}_{\text{støj}}}$. Er SNR høj er det et udtryk for meget \emph{lidt} støj i datasættet, men er SNR derimod lav er det et udtryk for meget støj i datasættet. I datasættet $X$ kan stor støj fremkomme, hvis eksempelvis vind påvirker bolden, hvorved den ikke hopper lodret op og ned men derimod svinger lidt fra side til side en gang i mellem. Dette ville betyde at den grafiske repræsentation, der burde være en lige linje, nærmere ikke længere vil være så ensformig. Støjen vil her være den vandrette varians [\citet{PCA_slens}, s. 3-4].

\subsection{Redundans} \label{sec:redundans}
Redundans er et udtryk for, at det samme element er beskrevet ved flere forskellige variable, der egentlig siger det samme. Redundans kan i bold-forsøget fremkomme ved at to kameraer sidder tæt ved siden af hinanden, hvorved de begge to vil have variable, men variablene giver næsten de præcis samme målinger. Dette kan repræsenteres ved et plot af $(x_{A},\ x_{B})$, som er hhv. x-koordinaten målt i kamera A og kamera B, og hvor kamera A og B er meget tæt placeret. Dette plot ville være tæt på en ret linje med forskrift $x=y$. Der er her en stor kovarians \footnote{Begrebet kovarians præsenteres i afsnit \vref{sec:kovarians}}, hvilket vil sige at de er lineært afhængige, og det er dermed spild af ressourcer at lave denne måling. Det giver også unødvendige data i datasættet, da der ikke udtrykkes andet eller mere præcis data ved den ekstra variabel. Redundans ønskes minimeret, da det er overflødig data og dermed blot fremstår som fyld i datasættet [\citet{PCA_slens}, s. 4]. 

\section{Kovariansmatrix}\label{sec:kovarians}
I sektion \vref{sec:stoj} blev SNR introduceret som værende et udtryk for mængden af støj i et datasæt, mens redundans i sektion \vref{sec:redundans} blev introduceret som værende kovariansen mellem to variable, dvs. relationen mellem de to variable. Disse vil blive uddybet i dette afsnit, men først vil ligningen for gennemsnittet blive introduceret.

\subsection{Gennemsnit}
Lad $\vec{x_i}$ være en \emph{række}vektor bestående af alle målinger, $x_1,\ldots,x_n$, i variablen. Gennemsnittet af målingerne betegnes ved $\mu_{x}$ og kan findes som værende\\
\begin{align}
\mu_{x} = \frac{1}{n}(x_1+ ... + x_n)
\label{eq:average}
\end{align}
Vektoren centreres omkring nul fremfor gennemsnittet $\mu$ ved at subtrahere gennemsnittet $\mu_{x}$ fra vektoren $\vec{x_i}$. Denne vektor angives som $\tilde{\vec{x_i}}$ og har gennemsnittet nul.
\begin{equation}
\tilde{\vec{x_i}} = \begin{bmatrix}
(x_1 - \mu_{x}) & \hdots & (x_n - \mu_{x})
\end{bmatrix}
\end{equation}

\subsection{Varians og kovarians}\label{sec:varians}
Varians er et udtryk for spredningen af data i en variabel. Spredningen, og dermed variansen i $\tilde{\vec{x_i}}$, kan findes som følgende prikprodukt
\begin{equation}
\sigma^2_{\tilde{\vec{x_i}}}= \frac{1}{n - 1}\tilde{\vec{x_i}} \cdot \tilde{\vec{x_i}}
\label{eq:varians}
\end{equation}
En høj varians er udtryk for stor spredning af dataene i $\tilde{\vec{x_i}}$, mens en lille varians er et udtryk for en lille spredning i dataene og dermed meget ensformig data i denne vektor, og dermed i den række data vektoren repræsenterer.
Det er ydermere interessant at se på, hvordan to rækkevektorer relaterer til hinanden, hvilket gøres ved beregning af kovariansen mellem de to vektorer. Kovariansen mellem vektorerne $\tilde{\vec{x_a}}$ og $\tilde{\vec{x_b}}$ kan beregnes som \begin{equation}
\sigma^2_{(\tilde{\vec{x_a}},\tilde{\vec{x_b}})} = \frac{1}{n - 1} \tilde{\vec{x_a}} \cdot {\tilde{\vec{x_b}}}^T
\label{eq:covarians}
\end{equation}
hvor $(\tilde{\vec{x_a}}) \cdot (\tilde{\vec{x_b}})^T$ er prikproduktet mellem de to vektorer. Kovariansen mellem de to vektorer udtrykker den lineære relation mellem de to vektorer. En høj kovarians er udtryk for at dataene relaterer meget til hinanden, mens en lille kovarians er udtryk for, at dataene er lidt relaterede. Er kovariansen $\sigma^2_{(\tilde{\vec{x_a}},\tilde{\vec{x_b}})} = 0$, er vektorerne fuldstændigt lineært urelaterede til hinanden. Det gælder at $\sigma^2_{(\tilde{\vec{x_a}},\tilde{\vec{x_b}})}=\sigma^2_{(\tilde{\vec{x_a}})}$ hvis $A=B$ [\citet{PCA_slens}, s. 5].

Undersøges der mange variable, som i eksperimentet med bolden, er der flere variable end blot to, og det kan være interessant at se på variansen for alle variable, såvel som kovariansen mellem alle variable. Dette er dog en uoverskuelig process, hvis det skal gøres ved ovenstående metode for alle variable, specielt hvis datasættet består af mange variable. Der kan derfor være fordelagtigt at udregne matrixproduktet
\begin{equation}
C_{\tilde{X}}= \frac{1}{n-1} \tilde{X}\tilde{X}^T, \phantom{mmmm} \text{hvor } \tilde{X}= \begin{bmatrix} \tilde{\vec{x_1}}  & \tilde{\vec{x_2}} & \hdots	 & \tilde{\vec{x_m}} \end{bmatrix}^T
\label{eq:matrix covarians}
\end{equation}
$\tilde{X}$ består af rækkevektorerne $\tilde{\vec{x_i}}=\begin{bmatrix} \tilde{x_1} & \tilde{x_2} & \hdots & \tilde{x_n} \end{bmatrix}$.
Det smarte ved $C_X$ er, at der ved hjælp af denne både beregnes variansen for alle vektorer såvel som kovariansen mellem alle vektorer. Dette ses tydeligt i den eksplicitte form af matrixproduktet
\begin{equation}
C_{\tilde{X}}=\frac{1}{n-1} \tilde{X}\tilde{X}^T = \frac{1}{n-1} 
\begin{bmatrix}
\tilde{\vec{x_1}} \cdot {\tilde{\vec{x_1}}}^T		& \tilde{\vec{x_1}} \cdot {\tilde{\vec{x_2}}}^T	&	\cdots	&	\tilde{\vec{x_1}} \cdot {\tilde{\vec{x_m}}}^T		\\
\tilde{\vec{x_2}} \cdot {\tilde{\vec{x_1}}}^T		& \tilde{\vec{x_2}} \cdot {\tilde{\vec{x_2}}}^T	&	\cdots	&	\tilde{\vec{x_2}} \cdot {\tilde{\vec{x_m}}}^T		\\
\vdots											& \vdots											&	\ddots	&			\vdots											\\
\tilde{\vec{x_m}} \cdot {\tilde{\vec{x_1}}}^T		& \tilde{\vec{x_m}} \cdot {\tilde{\vec{x_2}}}^T	&	\cdots	&	\tilde{\vec{x_m}} \cdot {\tilde{\vec{x_m}}}^T
\end{bmatrix}
\end{equation}
Det gælder her at variansen, for de respektive vektorer, er at finde i diagonalen, mens kovariansen mellem de enkelte variable kan findes udenfor diagonalen. Der er altså ved en enkelt beregning udregnet alle varianser og kovarianser for datasættet. Bemærk ydermere at matricen $C_{\tilde{X}}$ er en $m \times m$ symmetrisk matrix navngivet kovariansmatricen. $C_{\tilde{X}}$ er meget brugbar til hurtigt og nemt at se relationer i datasættet $\tilde{X}$, men det kan gøres endnu nemmere. Det ønskes at finde $C_Y$, som er en manipuleret kovariansmatrix, men før denne findes, skal egenskaberne for den defineres.

\subsection{Diagonalisering af kovariansmatricen}
Det er jævnfør afsnit \vref{sec:redundans} ønskværdigt at nedbringe redundans mest muligt, og da redundans er et udtryk for kovariansen mellem to vektorer, må det gælde, at kovariansen skal nedbringes til nul, hvorved dataene ikke relaterer til hinanden længere. Det gælder altså at alle indgange i $C_Y$, der ikke er i diagonalen skal være nul, hvorved $C_Y$ er en diagonalmatrix.

\subsubsection{Diagonalisering} \label{sec:diagonalisering}
En diagonaliserbar matrix er defineret ved følgende sætning:\\
En $n \times n$ matrix $A$ er diagonaliserbar, hvis og kun hvis $A$ har $n$ lineært uafhængige egenvektorer. I så fald gælder det at
\begin{align}
A = PDP^{-1}
\label{eq:diagonalisering}
\end{align}
hvor $P$ er en matrix med $A$'s egenvektorer som søjler, og $D$ er en diagonalmatrix med $A$'s egenværdier som indgange.
Ovenstående sætning bevises [\citet{linalg}, s. 315-316]. Lad:
\begin{align*} P = \begin{bmatrix} \vec{v_1} & \vec{v_2} & \hdots & \vec{v_n} \end{bmatrix} \end{align*}
\begin{align*} D = diag(\lambda_1, \lambda_2, …, \lambda_3) \end{align*}
Det gælder dermed at:
\begin{align*} AP = A\begin{bmatrix} \vec{v_1} & \vec{v_2} & \hdots & \vec{v_n} \end{bmatrix} = \begin{bmatrix} A\vec{v_1} & A\vec{v_2} & \hdots & A\vec{v_n} \end{bmatrix} \end{align*}
\begin{align*} PD = P\begin{bmatrix}
\lambda_1			\\
&	\lambda_2		\\
&&	\ddots			\\
&&&	\lambda_n
\end{bmatrix} = \begin{bmatrix} \lambda_1\vec{v_1} & \lambda_2\vec{v_2} & \hdots & \lambda_n\vec{v_n} \end{bmatrix} \end{align*}
Det antages, at A er diagonaliserbar med ligning \ref{eq:diagonalisering}, der omskrives til:
\begin{align*} AP = PD \end{align*}
\begin{align*} A\vec{v_1} = \lambda_1\vec{v_1},\ A\vec{v_2} = \lambda_2\vec{v_2},\ …,\ A\vec{v_n} = \lambda_n\vec{v_n} \end{align*}
$P$ er invertibel, har lineært uafhængige søjler, og det gælder at $ \vec{v_1} \neq 0,\ …\ ,\ \vec{v_n} \neq 0 $. Dette betyder, at søjlerne i $P$ består af de lineært uafhængige egenvektorer til $A$ med dertilhørende egenværdier $\lambda_1,\ \lambda_2,\ …,\ \lambda_n$. Dette kan yderligere bevises ved at tage beviset fra den anden vej:\\
Givet er $n$ lineært uafhængige egenvektorer $\vec{v_1}, \vec{v_2}, \hdots, \vec{v_n}$ med tilhørende egenværdier $\lambda_1, \lambda_2, …, \lambda_3$. Definerer $P$ og $D$ som ovenfor, og det følger dermed igen at $AP = PD$, da det gælder at:
\begin{align*} A\vec{v_1} = \lambda_1\vec{v_1},\ A\vec{v_2} = \lambda_2\vec{v_2},\ …,\ A\vec{v_n} = \lambda_n\vec{v_n} \end{align*}
Da $P$'s søjler er lineært uafhængige, er $P$ inverterbar, og det gælder derfor at
\begin{align*} A = PDP^{-1} \phantom{mmm} \blacksquare \end{align*}
Da det ønskes at diagonalisere $C_Y$, er det er altså vigtigt i ligning \vref{eq:linear transformation} at vælge en $P$, således at $C_Y$ er en diagonalmatrix.

%En diagonaliserbar matrix, er defineret som følgende: En $n \times n$ matrix $A$ er kun diagonaliserbar hvis der findes en base for $\mathbb{R}^n$, som består af egenvektorerne for $A$ [\citet{linalg}, s. 315].
%Derfor gælder det at $A = PDP^{-1}$ hvor $D$ er en diagonal matrix og $P$ er en invertibel matrix, hvis og kun hvis $P$ er en base for $\mathcal{R}^n$ bestående af egenvektorerne til A.\citep{linalg}
%

\section{Udregn PCA} \label{sec:UdregnPCA}
I de foregående afsnit er baggrunden for PCA blevet undersøgt og begrundet. PCA vil i dette afsnit blive fundet algebraisk for en $m\times n$ matrix $X$, der som i eksemplet med bolden har $m$ variable og $n$ målinger. Lad $P$ være den ortonormale $m \times m$ matrix $P$, hvor $Y=PX$ således at $C_Y=\frac{1}{n-1}YY^T$ er diagonaliseret, og \emph{rækkerne} i $P$ er principale komponenter til $X$. \\
Først omskrives $C_Y$, til at være et udtryk af $P$ frem for $Y$. Dette gøres med henblik på at diagonalisere $C_Y$.
\begin{align}
C_Y 	& = \frac{1}{n-1} YY^T						\\
	& = \frac{1}{n-1} (P\tilde{X})(P\tilde{X})^T		\nonumber	\\
	& = \frac{1}{n-1} P\tilde{X}\tilde{X}^TP^T		\nonumber	\\
	& = \frac{1}{n-1} P(\tilde{X}\tilde{X}^T)P^T		\nonumber	\\
	& = \frac{1}{n-1} PSP^T
\end{align}
Det gælder her, at $S$ er en symmetrisk $m \times m$ matrix, hvilket kan ses ved $(XX^T)^T = (X^T)^TX^T = XX^T$. Ifølge [\citet{PCA_people}, s. 8] og afsnit \ref{sec:diagonalisering} er enhver symmetrisk matrix ortogonal diagonaliserbar og dermed også ortonormal diagonaliserbar (ortogonale med længden én). Dermed kan $S$ udtrykkes ved 
\begin{align*}
S=EDE^{-1}
\end{align*}
hvor $D$ er diagonalmatricen, hvor indgangene i diagonalen er egenværdierne til $S$ ordnet efter $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m \geq 0$ \footnote{Se bevis for at egenværdierne er positive i appendiks \ref{app:eigenvalues}}. $E$ er en $m \times m$ ortonormal matrix, hvis \emph{søjler} er normaliserede egenvektorer tilhørende egenværdierne og følger samme ordning som egenværdierne, hvorved egenvektoren, til den største egenværdi, står som første søjle i $E$. 
Da $E$ er ortonormal, gælder det at $E^{-1}=E^T$ [\cite{linalg}, s. 413] og \ref{eq:SEDET} kan udtrykkes som
\begin{align}
S=EDE^T
\label{eq:SEDET}
\end{align}

%\textbf{Eksempel}\\
%En tilfældig symmetrisk matrix, $S$, er lavet. 
%\begin{align}
%S=\begin{bmatrix}
%3	&	2	&	4\\
%2	&	0	&	2\\
%4	&	2	&	3
%\end{bmatrix}
%\end{align}
%Dens egenvektorer og -værdier beregnes og matricerne $E$ og $D$ kan laves ud fra disse.
%\begin{align}
%E=\begin{bmatrix}
%1			&	-1	&	-\frac{1}{2}\\
%\frac{1}{2}	&	0	&	1			\\
%1			&	1	&	0
%\end{bmatrix}
%D=\begin{bmatrix}
%8	&	0	&	0	\\
%0	&	-1	&	0	\\
%0	&	0	&	-1
%\end{bmatrix}
%\end{align}
%Altså kan $S$ diagonaliseres således
%\begin{align}
%\begin{bmatrix}
%3	&	2	&	4\\
%2	&	0	&	2\\
%4	&	2	&	3
%\end{bmatrix}
%=
%\begin{bmatrix}
%1			&	-1	&	-\frac{1}{2}\\
%\frac{1}{2}	&	0	&	1			\\
%1			&	1	&	0
%\end{bmatrix}
%\cdot
%\begin{bmatrix}
%8	&	0	&	0	\\
%0	&	-1	&	0	\\
%0	&	0	&	-1
%\end{bmatrix}
%\cdot
%\begin{bmatrix}
%1				&	\frac{1}{2}	&	1			\\
%-1				&	0			&	1			\\
%-\frac{1}{2}	&	1			&	0
%\end{bmatrix}
%\end{align}
%Derved er matricen $S$ ortogonalt diagonaliseret.

Egenvektorer og egenværdier er defineret som følgende: Lad $T$ være en lineær transformation hvor domænet og kodomænet er lig $\mathbb{R}^n$, hvilket er en lineær operator på $\mathbb{R}^n$. En vektor $\vec{v}$ forskellig fra nul i $\mathbb{R}^n$ benævnes som en egenvektor af $T$, hvis $T(\vec{v})$ er et produkt af $\vec{v}$; dvs.
\begin{align}
T(\vec{v})=\lambda\vec{v}
\end{align}
for skalarerne $\lambda$. Skalarerne $\lambda$ benævnes som egenværdier af $T$, som tilhører $\vec{v}$. [\citet{linalg}, s. 294]

Egenværdierne ved en kvadratisk matrix $A$, er værdierne af $\lambda$ som tilfredsstiller
\begin{align}\label{eq:karakteristiskligning}
\det(A - I_m \cdot \lambda ) = 0
\end{align}
i forhold til $\lambda$ og herefter indsættes disse i diagonalen i $D$.
Udtryk \ref{eq:karakteristiskligning} kaldes den karakteristiske polynomium af $A$, mens $det(A - I_m \cdot \lambda)$ kaldes det karakteristiske polynomium af $A$ [\citet{linalg}, s. 302].

Determinanten, som fremgår i førnævnte udtryk, har nyttige egenskaber i lineær algebra, som eksempelvis at finde egenværdier. Ved matricer hvor $n \geq 3$, hvilket gør sig gældende ved de billeder, der undersøges i projektet, er determinanten defineret ved en $n \times n$ matrix $A$ ved første række som
\begin{align*}
\textrm{det}A=a_{11}\cdot \textrm{det}A_{11} - a_{12} \cdot \textrm{det}A_{12}+\cdots +(-1)^{1+n}a_{1n}\cdot \textrm{det}A_{1n}
\end{align*}
Ved at lade $c_{ij} = (-1)^{i+j}\cdot\textrm{det}A_{ij}$ kan definitionen af determinanten af $A$ skrives som
\begin{align*}
\textrm{det}A=a_{11}c_{11}+a_{12}c_{12}+\cdots+a_{1n}c_{1n}
\end{align*}
Dette udtryk er kofaktor ekspansionen af $A$ hen ad første række.
Et generelt eksempel på at finde egenværdier følger; lad $S$ være en kendt matrix i $\mathbb{R}^3$. Det ønskes at finde løsningen til den ubekendte $\lambda$. Udtrykket $\textrm{det}(A - I_m \cdot \lambda )$ opstilles.
\begin{align*}
\textrm{det}(A - I_m \cdot \lambda) = \text{det}\begin{bmatrix}
a_{11} - \lambda & a_{12} & a_{13}\\
a_{21} & a_{22} - \lambda & a_{23}\\
a_{31} & a_{32} & a_{33} - \lambda\\
\end{bmatrix}
\end{align*}
Ved at benytte kofaktor ekspansion hen ad første række fås et ligningssystem, som ved de mulige løsninger udtrykker egenværdierne. 
De tilhørende egenvektorer findes som løsninger til
\begin{align*}
(A - I_m \cdot \lambda) \cdot \vec{v} = 0
\end{align*} for de respektive $\lambda$-værdier og indsættes som søjlevektorer i $E$. I praksis vil det sige at løse ligningssystemet til
\begin{align*}
\begin{bmatrix}
a_{11} - \lambda & a_{12} & a_{13}\\
a_{21} & a_{22} - \lambda & a_{23}\\
a_{31} & a_{32} & a_{33} - \lambda\\
\end{bmatrix}
\begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3} \\
\end{bmatrix}=
\begin{bmatrix}
0\\0\\0\\
\end{bmatrix}
\end{align*}
hvilket blot gøres ved at gå til reduceret trappeform med $A$ og opstille ligningssystemet på parametriseret vektorform. Vektorerne danner en basis med de tilhørende egenvektorer for egenrummet af $A$. Dette er dog beregningsmæssigt så tungt (specielt med matricer større end $3 \times 3$, at det i praksis kun vil blive gjort vha. et digitalt beregningsværktøj. De egenvektorer, der fremkommer af ovenstående beregninger, er dog kun ortogonale og ikke ortonormale, som det ønskes i $E$. For at lave dem ortonormale normaliseres de til længden én ved den indgangsvise division med længden af sig selv
\begin{align}
\hat{\vec{v_i}} = \frac{v_i}{\Vert\vec{v_i}\Vert}
\label{eq:ortonormal}
\end{align}

Pr. definition er $E$'s søjler lineært uafhængige, ortogonale (nu ortonormale) og af dimension $m$ [\citet{linalg}, s. 315,426], da dette er egenskaberne for egenvektorer.

Transformationsmatricen $P$ defineres, og det ses, at det er praktisk at sætte $P = E^T$ og dermed $P^T = E$. Dette betyder, at \emph{rækkerne} i $P$ er søjlevektorerne i $E$, og disse er netop blevet defineret som værende egenvektorerne til $S$. $E$ er ortonormal, og det gælder derfor at $E^TE = I_m$, hvor $I_m$ er $m \times m$ identitetsmatricen. Ligning \ref{eq:CY omskrivning} kan vha. ligning \ref{eq:SEDET} omskrives til
\begin{align}
C_Y 	& = \frac{1}{n-1} PSP^T \		\nonumber	\\
	& = \frac{1}{n-1} P(EDE^T)P^T	\nonumber	\\
	& = \frac{1}{n-1} E^TEDE^TE		\nonumber	\\
	& = \frac{1}{n-1} D
\label{eq:CY}
\end{align}
Det ses her, at $C_Y$ er blevet diagonaliseret når $P=E^T$, hvilket var målet med PCA. I de foregående afsnit er det fremkommet, at principal komponenterne til $X$ er \emph{rækkerne} i $P$. Det ses i ligning \ref{eq:CY}, at den $i$'te diagonalindgang er variansen af $X$ i den nye basis $P$, hvilket imidlertid betyder, at egenværdierne i $D$ udtrykker variansen af de forskellige principal komponenter og dermed deres indflydelse på datasættet [\citet{PCA_people}, s. 8].




%\section{Baggrund i statistik}
%En stor del af PCA er også en statistisk del, der har til formål at skelne de vigtigste elementer fra de mindre vigtige elementer. I forhold til statistik i PCA ønskes der at fokuseres på tre forskellige parametre; gennemsnittet, varians og kovarians, der alle giver et indblik i dataenes relationer.
%
%I de følgende sektioner vil der blive arbejdet med en $m \times n$ matrix $X$, hvor $n$ angiver mængden af individer eller prøver og $m$ angiver antal variable, der er testet. Det ønskes at lave en lineær transformation på $m \times n$ matricen $X$ til $m \times n$ matricen $Y$ for en $n \times m$ matrix $P$. Dette udtrykkes som
%\begin{equation}
%Y=PX
%\end{equation}
%Der sker en ændring i basen for matricen $X$ og rækkerne i $P$ kan angives som rækkevektorerne $p_1,p_2,…p_m$, og søjlerne i $X$ kan udtrykkes som søjlevektorerne $x_1,x_2,…,x_n$. Rækkerne i $P$ er en ny basis for rækkerne i $X$, og vil i løbet af de følgende afsnit blive til retningerne på principal components \fixme{dansk navn}. \fixme{Ovenstående er noget rod}
%
%PCA er en metode til at finde relationen mellem vektorerne i den oprindelige basis?
%
%\subsection{Gennemsnit} \label{sec:gennemsnit}
%Lad $x_1,…,x_n$ være en rækkevektor betående af alle svar på et spørgsmål $m$. Gennemsnittet af svarene kan dermed findes som værende
%\begin{align}
%\mu_X = \frac{1}{n}(x_1+ ... + x_n)
%\label{eq:average}
%\end{align}\fixme{mean vs sample average?}
%Ligning \ref{eq:average} giver et udtryk for hvor svarene/dataene er centreret, men det kan have stor interessse også at se på, hvor spredte disse svar er. Dette findes som værende variansen i dataene og præsenteres i næste afsnit.
%
%\subsection{Varians}
%Varians er et udtryk for forskelligheden i et datasæt, og vil i eksemplet fra sektion \vref{sec:gennemsnit} være variansen i de svar, de adspurgte personer har givet. Variansen kan findes på baggrund af datasættet ved formlen
%\begin{align}
%Var(A)= \frac{1}{n - 1}((a_1 - \mu_A)^2 + ... + (a_n - \mu_A)^2)
%\label{eq:varians}
%\end{align}\fixme{kilde? - n-1 eller n afhænger om sample eller population}
%Denne giver et udtryk for hvor stor spredning, der er i datasættet. En høj varians er en stor spredning og en lille varians giver udtryk for at dataene ligger tæt samlet. Variansen kan findes i alle variable som måles i et datasæt.
%
%\subsection{Kovarians}
%Variansen giver et udtryk for hvor spredt et datasæt ligger, mens kovariansen udtrykker relationen mellem to datasæt. Måles to parametre $A$ og $B$ i et datasæt, kan relationen mellem disse to parametre findes ved kovariansen. Er kovariansen negativ betyder det at hvis $A$ bliver større, så bliver $B$ mindre. Kovariansen findes ved formlen
%\begin{align}
%Cov(A,B) = \frac{1}{n - 1}((a_1 - \mu_A)(b_1 - \mu_B) + ... + (a_n - \mu_A)(b_1 - \mu_B))
%\label{eq:covarians}
%\end{align}\fixme{kilde}
%Bemærk her at $Cov(A,B)=Cov(B,A)$ og at kovariansen kan findes mellem to variable, blandt alle målte, såfremt flere måles. \fixme{Kovariansen har betydning for PCA, da det er interessant at finde de variable med størst kovarians, da disse bedst kan beskrive den store mængde af data.}\fixme{betydning for PCA}

%1.
%$(A-\lambda I_n )  x=0   ,x /=0
%I ovenstående tilfælde gælder det at løsningssættet er eigenrummet til A i forhold til eigenværdien \lambda. Dette er blot nulrummet til A-\lambda I_n.$
%2. 
%$Eigenværdierne til en kvadratisk matrix A er værdierne t, der løser:
%det(A-tI_n )=0
%Ovenstående ligning er kaldet ”the characteristic equation” til A. Og det(A-tI_n )   er kaldet ”the characteristic polynomial” til A.$
%3.
%$To similære matricer A og B har samme karakteristiske polynomium da
%det(B-\lambda I_n )=detP  det(B-\lambda I_n )*detP^(-1) 
%det(B-\lambda I_n )=det(P(B-\lambda I_n ) P^(-1) )=det(A-\lambda I_n )$
%4.
%$Hvad har det af betydning for PCA?$
%-----------

\section{Billedekomprimering med PCA}
PCA-metoden kan anvendes på mange forskellige former for data med forskellige formål, men jævnfør rapportens formål undersøges PCA i forhold til komprimering af et billede. Billedet angives som tre $m \times n$ matricer indeholdende pixelintensiteten i de forskellige indgange, der repræsenterer en pixel i billedet.

For overskuelighedens skyld undersøges her et gråtone-billede med $10 \times 8$ pixels. Billedet betegnes ved matricen $X$. Udgangspunktet for de følgende regneoperationer er udtrykket $Y = PX$, hvor $Y$ er den transformerede matrix, og $P$ udgør en matrix med egenvektorerne til $C_X$ som \emph{rækker}, hvilket reelt set benævnes principale komponenter til $X$ (fremover vil principale komponenter forkortes med PC). Det skal nævnes, at efter eksemplet udføres udregningerne på Lena, dog vil kun resultaterne af Lena fremgå. Dette gøres for at danne grundlag for sammenligning i forhold til DCT-billedkomprimeringen.
\begin{figure}[!h]
\begin{minipage}[b]{0.27\linewidth}
\centering
\includegraphics[width=0.62\textwidth]{Billeder/LenaAnvendelse/testBillede/testBillede1.png}
\caption{Oprindelig - Visuel.}
\label{fig:X_anvendelse-tal-start}
\end{minipage}
\hspace{0.5cm}
\resizebox{0.32\textwidth}{!}{\begin{minipage}[b]{0.45\linewidth}
\centering
\[X = \begin{bmatrix}
187 & 207 & 213 & 217 & 213 & 194 & 172 & 160 \\
210 & 236 & 236 & 219 & 202 & 170 & 147 & 148 \\
173 & 210 & 230 & 212 & 171 & 135 & 126 & 138 \\
167 & 190 & 203 & 171 & 123 & 106 & 119 & 125 \\
204 & 217 & 207 & 167 & 127 & 107 & 99 & 81 \\
249 & 255 & 231 & 197 & 169 & 132 & 91 & 55 \\
255 & 254 & 234 & 214 & 193 & 163 & 117 & 66 \\
253 & 244 & 234 & 224 & 201 & 177 & 142 & 89 \\
251 & 237 & 235 & 229 & 209 & 193 & 161 & 105 \\
242 & 230 & 232 & 235 & 225 & 214 & 180 & 118
\end{bmatrix}\]
\caption{Oprindelig - Tal.}
\label{fig:X_anvendelse-tal-start1}
\end{minipage}}
\end{figure}

For at skabe et overblik over billedkomprimeringen med PCA ses algoritmen herunder med uddybbelse og anvendelse på $X$ se figur \ref{fig:X_anvendelse-tal-start1}.

\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\multicolumn{3}{l}{\textbf{Algoritme: Komprimering vha. PCA}}\\                                                             \hline
\\
\multicolumn{1}{|l}{1.}        & Input:                     & Billede, $X$: $m \times n$ pixels                                   \\
\multicolumn{1}{|l}{2.}        & Output:                    & Komprimeret fil                                           \\
                               &                            &                                                                   \\
\multicolumn{2}{|l}{\textit{Komprimering}}                 &                                                                   \\
\multicolumn{1}{|l}{3.}        & Gennemsnit:                  & $\mu_{x} = \frac{1}{n}(x_1+ ... + x_n)$           \\
\multicolumn{1}{|l}{4.}        & Standardiserer:			 & 
$\tilde{\vec{x_i}} = \begin{bmatrix} (x_1 - \mu_{x}) & \hdots & (x_n - \mu_{x}) \end{bmatrix}$          \\
\multicolumn{1}{|l}{5.}        & Kovarians matrix:              & 
$C_{\tilde{X}}=\frac{1}{n-1} \tilde{X}\tilde{X}^T , \phantom{mmm} hvor \tilde{X}= \begin{bmatrix} \tilde{\vec{x_1}} &		\hdots & \tilde{\vec{x_m}}  \end{bmatrix}^T$           \\
\multicolumn{1}{|l}{6.}        & Sortering:              & Egenvektorer, $P$, sorteres vha. egenværdier \\
\multicolumn{1}{|l}{7.}        & PC vælges:    			& 
Antal bibeholdte principale komponenter afgør kvalitet  \\
\multicolumn{1}{|l}{8.}        & Transformerer matrix          & 
$Y = PX$            \\
\multicolumn{1}{|l}{9.}        & Entropikodning:        & $Y$, $P$ og $\mu_{\vec{x}}$ gemmes i en fil vha. Huffman     \\
\end{tabular}
\label{tb:Algoritme-Komprimering-PCA}
\end{table}

Med udgangspunkt i $X$ findes gennemsnittene for de enkelte rækkevektorer $\vec{x_i}$ som derefter trækkes fra de respektive rækkevektorer. Jævnfør vektoren \vref{eq:X_anvendelse} ses det, at gennemsnittet for første rækkevektor er $\frac{1}{8} (187 + 207 + 213 + 217 + 213 + 194 + 172 + 160) = 195,38$.

For at skabe indblik i følgende regnemetode stilles resultatet for samtlige gennemsnit op i en såkaldt gennemsnitsvektor, der indeholder alle gennemsnittene for de enkelte rækker
\begin{equation}
\mu_{\vec{x}} = \begin{bmatrix}
195,38 & 196 & 174,375 & 150,5 & 151,125 & 172,375 & 187 & 195,5 & 202,5 & 209,5 
\end{bmatrix}^T
\label{eq:X_anvendelse}
\end{equation}
Gennemsnittet trækkes fra de respektive rækker i $X$ - dvs. $X$ standardiseres og resulterer i følgende matrix:
\begin{equation}\resizebox{.7\textwidth}{!}{$
\tilde{X}=\begin{bmatrix}
-8,375 & 11,625 & 17,625 & 21,625 & 17,625 & -1,375 & -23,375 & -35,375 \\
14 & 40 & 40 & 23 & 6 & -26 & -49 & -48 \\
-1,375 & 35,625 & 55,625 & 37,625 & -3,375 & -39,375 & -48,375 & -36,375 \\
16,5 & 39,5 & 52,5 & 20,5 & -27,5 & -44,5 & -31,5 & -25,5 \\
52,875 & 65,875 & 55,875 & 15,875 & -24,125 & -44,125 & -52,125 & -70,125 \\
76,625 & 82,625 & 58,625 & 24,625 & -3,375 & -40,375 & -81,375 & -117,375 \\
68 & 67 & 47 & 27 & 6 & -24 & -70 & -121 \\
57,5 & 48,5 & 38,5 & 28,5 & 5,5 & -18,5 & -53,5 & -106,5 \\
48,5 & 34,5 & 32,5 & 26,5 & 6,5 & -9,5 & -41,5 & -97 \\
32,5 & 20,5 & 22,5 & 25,5 & 15,5 & 4,5 & -29,5 & -91,5 \\
\end{bmatrix}$}
\label{eq:X_tilde}
\end{equation}
Den nye matrix $\tilde{X}$ består af \emph{rækkevektorerne} $\vec{\tilde{x_i}}$, der nu er centreret omkring nul. Kovariansmatricen $C_{X}$ findes vha. ligning \ref{eq:matrix covarians} med dimensionerne $m \times m$, hvor $m = 10$, hvilket ses ved, at dimensionerne er $(10 \times 8) \cdot (8 \times 10) = 10 \times 10$.
\begin{equation}\resizebox{.7\textwidth}{!}{$
C_X = \begin{bmatrix}
449 & 654 & 667 & 419 & 720 & 1156 & 1116 & 944 & 826 & 742\\
654 & 1336 & 1375 & 1164 & 1839 & 2563 & 2288 & 1881 & 1565 & 1226\\
667 & 1375 & 1568 & 1336 & 1832 & 2405 & 2091 & 1716 & 1410 & 1062\\
419 & 1164 & 1336 & 1339 & 1820 & 2214 & 1846 & 1499 & 1199 & 808\\
720 & 1839 & 1832 & 1820 & 2943 & 3916 & 3430 & 2817 & 2322 & 1728\\
1156 & 2563 & 2405 & 2214 & 3916 & 5533 & 4992 & 4132 & 3465 & 2728\\
1116 & 2288 & 2091 & 1846 & 3430 & 4992 & 4586 & 3824 & 3247 &   2637\\
944 & 1881 & 1716 & 1499 & 2817 & 4132 & 3824 & 3211 & 2744 & 2257\\
826 & 1565 & 1410 & 1199 & 2322 & 3465 & 3247 & 2744 & 2367 & 1985\\
742 & 1226 & 1062 & 808 & 1728 & 2728 & 2637 & 2257 & 198 & 1746\\
\end{bmatrix}$}
\end{equation}

Da det gælder at $S=\tilde{X}\tilde{X}^T$, og endvidere som tidligere nævnt $S = EDE^T$, findes egenværdierne og egenvektorerne vha. det karakteristiske polynomium til kovariansmatricen hvor $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m \geq 0$. Egenvektorerne sorteres ikke, men tilrettes samme ordning som egenværdierne, da egenværdierne og egenvektorerne hænger sammen i par. Metoden til at finde $S$, $E$ og $D$ såvel som egenværdier og egenvektorer ses i afsnit \ref{sec:UdregnPCA}. For forståelsens skyld kan egenværdierne opstilles som en diagonal matrix, $D$, med tilhørende egenvektorer som matricerne $E$ og $E^T$ og dermed udregne $S$, dette gøres dog ikke i denne rapport. Dette ville resultere i kovariansmatricen.
Sorterede egenværdier findes, og egenvektorer opstilles som søjler: 
\begin{align*}
\lambda_1 = 2,3 \cdot 10^4,\phantom{m}\lambda_2 = 1,43 \cdot 10^3 ,\phantom{m}\lambda_3 = 530,\phantom{m}\lambda_4 = 10,\phantom{m}\lambda_5 = 7,35,\phantom{m}\lambda_6 = 2,22,\\
\phantom{m}\lambda_7 = 0,6,\phantom{m}\lambda_8 = 5 \cdot 10^{-14},\phantom{m}\lambda_9 = -3,8 \cdot 10^{-13},\phantom{m}\lambda_{10} = -8,7 \cdot 10^{-14}
\end{align*}
\begin{align}
E = 
\left\{
\begin{bsmallmatrix}
0,11  \\
0,07  \\
0,55  \\
0,16  \\
-0,52 \\
-0,36 \\
-0,23 \\
-0,30 \\
0,28  \\
0,06  \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,23  \\
-0,19 \\
0,30  \\
0,40  \\
0,03  \\
0,25  \\
0,73  \\
-0.02 \\
-0,03 \\
0,12  \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,22  \\
-0,45 \\
0,54  \\
-0,06 \\
0,34  \\
0,15  \\
-0,37 \\
0,006 \\
0,03  \\
-0,33 \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,20  \\
-0,54 \\
-0,06 \\
-0,49 \\
-0,07 \\
-0.30 \\
0,17  \\
0,36  \\
-0,35 \\
0,50  \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,34  \\
-0,32 \\
-0,39 \\
-0,15 \\
-0,45 \\
0,16  \\
-0.07 \\
-0,46 \\
0,41  \\
-0,44 \\
\end{bsmallmatrix}\right., \nonumber \\ 
\left.
\begin{bsmallmatrix}
0,49   \\
-0,02  \\
-0,28  \\
0,41   \\
0,04   \\
0,30   \\
-0,29  \\
0,08   \\
-0,004 \\
0,20   \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,44   \\
0,20   \\
-0,094 \\
0.25   \\
-0,09  \\
-0.54  \\
0,05   \\
0,42   \\
-0,40  \\
-0,08  \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,37   \\
0,23   \\
-0,024 \\
-0,14  \\
0,45   \\
-0.08  \\
-0,21  \\
-0,12  \\
-0,03  \\
0,34   \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,31  \\
0,29  \\
0,06  \\
-0,34 \\
0,27  \\
-0,17 \\
0,34  \\
-0,48 \\
0,57  \\
-0,49 \\
\end{bsmallmatrix},
\begin{bsmallmatrix}
0,25  \\
0,43  \\
0,26  \\
-0,42 \\
-0,35 \\
-0,50 \\
0,01  \\
0,37  \\
-0,36 \\
0,20  \\
\end{bsmallmatrix}
\right\}
\end{align}
Ovenstående egenvektorer er ortonormale, da de er udregnet vha. Python, der automatisk omregner dem til ortonormale.\\
Som forklaret tidligere i afsnit \vref{sec:UdregnPCA}, kan $P$ udtrykkes som $P = E^T$. Hermed kan $Y$ udregnes. På dette tidspunkt i processen kan principale komponenter fjernes, hvor der typisk bibeholdes de komponenter, som udtrykker en stor andel af billedet - i praksis reducereres rækkerne af $P$ fra $m$ til $k$, hvorved dimensionen for $P$ bliver $k \times m$. Det er dette trin, der komprimerer billedet, da der nu skal gemmes færre værdier for at gengive det tilnærmelsesvist samme billede. Der kan beregnes, hvor meget en enkelt principal komponent udgør af billedet ud fra den tilhørende egenværdi og summen af alle egenværdier. Eksempelvis udgør den første principal komponent i dette tilfælde $\frac{1,61\cdot10^5}{1,76\cdot10^5} \approx 91,7\%$, den anden $\frac{10^4}{1,76\cdot10^5} \approx 5,7\%$ etc.

Ved dette regneeksempel vælges to principale komponenter til videre udregning. Typisk vælges antallet alt efter hvornår flere principale komponenter ikke ændrer billedet drastisk. Det er det essentielle ved metoden, da det ofte muliggør en fjernelse af $n$-dimensioner dvs. en høj data reduktion. \\
Videre i udregningen stilles de to valgte principale komponenter op som en $2 \times 10$ matrix. Dimensionerne for transformationen kan opskrives som $(k \times n) = (k \times m) \cdot (m \times n)$.
Den transformerede matrix findes vha. ligning \ref{eq:linear transformation}, $Y = P \cdot \tilde{X}$ med dimensionerne $k \times n$, ($2 \times 10) \cdot (10 \times 8) = 2 \times 8$.
\begin{equation}\resizebox{.7\textwidth}{!}{$
Y = \begin{bmatrix}
135.42 & 152.50 & 132.28 & 74.51 & 2.25 & -77.74 & -159.33 & -255.39 \\
23.95 & -24.89 & -42.85 & -6.82 & 35.03 & 52.36 & 15.42 &  -52.21
\end{bmatrix}$}
\end{equation}

Jævnfør algoritmen på side \pageref{tb:Algoritme-Komprimering-PCA}, gemmes $Y$, $P$ og gennemsnitsvektoren i en komprimeret fil. Filen er komprimeret, da mængden som gemmes, er mindre efter transformationen end før. Fra 80 datapunkter, $X: 10 \times 8$ - til 46 datapunkter, $Y: 2 \times 8$, $P: 2 \times 10$ og $\mu_{\vec{x}}: 10 \times 1$. Grundet reduceringen af dimensionerne komprimeres billedet med tab. For at komprimere filen yderligere Huffmankodes den.
\subsection*{Huffman}
Det komprimerede billede, repræsenteret ved matricerne $Y$ og $P$ samt gennemsnitsvektoren $\vec{\mu}_X$ Huffmankodes, for at udtrykke det ved en binærstreng, som der blev gjort ved billedet komprimeret med DCT i afsnit \vref{sec:huffmanteori}.

Det bemærkes, at der ikke nødvendigvis opnås en yderligere komprimering af billedet ved Huffmankodning. Dette skyldes, at $Y$, $P$ og $\vec{\mu}_X$ ikke nødvendigvis indeholder lignende værdier, som er tilfældet med billeder behandlet med DCT. Det kan derimod lade sig gøre, at en Huffmankodet fil fylder mere, end filen der komprimeres uden Huffman. Der benyttes derfor udelukkende Huffmankodning til PCA for at kunne sammenligne de to datasæt fra DCT og PCA.

\subsection*{Dekomprimering} \label{PCA_dekomprimering}
Dekomprimeringen udføres vha. algoritmen på side \pageref{tb:Algoritme-Dekomprimering-PCA}.
\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\multicolumn{2}{l}{\textbf{Algoritme: Dekomprimering vha. PCA}}    &                                                                   \\ \hline
\\
\multicolumn{1}{|l}{1.}        & Input:                     & Komprimeret fil \\
\multicolumn{1}{|l}{2.}        & Output:                    & Dekomprimeret billede, $\hat{X}$ \\
                               &                            &                                                                   \\
\multicolumn{2}{|l}{\textit{Dekomprimering}}                 &                                                                   \\
\multicolumn{1}{|l}{3.}        & Dekodning:        		 & Filen aflæses til $Y$, $P$ og $\mu_{\vec{x}}$           \\
\multicolumn{1}{|l}{4.}        & Tilbage transformerer matrix & $\hat{\tilde{X}} = P^T Y$           \\
\multicolumn{1}{|l}{5.}        & Destandardiserer:			 & 
$\hat{X} = \hat{\tilde{X}} + \mu_{\vec{x}}$          \\
\end{tabular}
\label{tb:Algoritme-Dekomprimering-PCA}
\end{table}

Det ses at efter dekomprimeringen transformeres matricen vha.
\begin{align}
\hat{\tilde{X}} = P^T Y
\label{eq:PCAinvers}
\end{align}
Dette kan lade sig gøre, grundet $P$s egenskaber som en højreinvertibel matrix.
\begin{itemize}
\item[]{En $m \times n$ matrix $A$, hvor $Rank(A)=m$, er højreinvertibel og har en højreinvers matrix $A^{-1}$ defineret ved [\citet{leftinverse}, s. 1]
\begin{align}
A^{-1}=A^T(AA^T)^{-1}
\label{eq:hoejreinvers}
\end{align} 
}
\end{itemize}
$P$ har ortonormale rækker, og derfor gælder der, at
\begin{align}
PP^T=I
\end{align}
Hvor $I$ er identitetsmatricen. Derfor fremkommer, jævnfør ligning \ref{eq:hoejreinvers}, at
\begin{align}
P^T(PP^T)^{-1}=P^T
\end{align}
Det ses, at for den ikke-kvadratiske ortnormale matrix $P$, er den transponerede lig den inverse. Altså kan $\hat{\tilde{X}}$ beregnes ved ligning \ref{eq:PCAinvers}.
Efter dette skridt standardiseres den, dvs. den centreres tilbage omkring gennemsnittet, ved at addere alle søjler med $\mu_{\vec{x}}$. Resultatet af matricen ses herunder.

\begin{figure}[!h]
\begin{minipage}[b]{0.27\linewidth}
\centering
\includegraphics[width=0.62\textwidth]{Billeder/LenaAnvendelse/testBillede/testBillede2.png}
\caption{Dekomprimeret - Visuel.}
\label{fig:X_anvendelse-tal-slut}
\end{minipage}
\hspace{0.5cm}
\resizebox{0.32\textwidth}{!}{\begin{minipage}[b]{0.45\linewidth}
\centering
\[\hat{X} = \begin{bmatrix}
212 & 210 & 207 & 203 & 197 & 190 & 179 & 164 \\
223 & 236 & 235 & 214 & 189 & 168 & 156 & 147 \\
193 & 219 & 223 & 194 & 158 & 133 & 132 & 142 \\
164 & 194 & 200 & 169 & 131 & 107 & 111 & 128 \\
190 & 211 & 210 & 179 & 139 & 108 &  91 &  80 \\
238 & 247 & 238 & 209 & 170 & 133 &  94 &  49 \\
252 & 250 & 237 & 219 & 193 & 163 & 119 &  63 \\
251 & 246 & 235 & 221 & 203 & 179 & 140 &  89 \\
252 & 243 & 231 & 224 & 212 & 193 & 157 & 108 \\
253 & 237 & 224 & 225 & 224 & 212 & 176 & 124 \\
\end{bmatrix}\]
\caption{Dekomprimeret - Tal.}
\label{fig:X_anvendelse-tal-slut1}
\end{minipage}}
\end{figure}

Regneeksemplet er lavet for at give et indblik i algoritmen for billedkomprimering med PCA. Ved metodens anvendelse på et farvebillede, gentages processen for hver farvekanal, hvilket er tilfældet ved Lena.
Metoden anvendes nu på Lena, og repræsentationer af Lena med forskellige valg af principale komponenter kan ses i bilagsmappen.